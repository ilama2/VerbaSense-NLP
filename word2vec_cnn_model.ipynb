{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql4X3mfDaDvT",
        "outputId": "197f793f-961c-4f2b-ad62-e95c9b812344"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, gensim\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "socOo24qZxxR",
        "outputId": "51d0879e-805a-4a08-c049-d9ae89c0157a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load dataset\n",
        "url = \"https://raw.githubusercontent.com/ironhack-labs/project-nlp-challenge/refs/heads/main/dataset/data.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "#drop unnecessary columns\n",
        "df = df.drop(columns=['date','subject','title'],axis=1)\n",
        "\n",
        "#fill missing values in the text column\n",
        "df['text'] = df['text'].fillna(\"\")\n"
      ],
      "metadata": {
        "id": "F3oCuWZ6Z81v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Pre-trained Word2Vec Model\n",
        "\n"
      ],
      "metadata": {
        "id": "VMzUXTk9gDx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load pre-trained Word2Vec model (Google News 300)\n",
        "w2v_model = api.load(\"word2vec-google-news-300\")\n",
        "embedding_dim = 300  #word vector size\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hk9JKb0Z-xu",
        "outputId": "37f08063-2780-424d-869b-0b4f94253af8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function to convert text into a list of word vectors\n",
        "def text_to_sequence(text, model):\n",
        "    words = word_tokenize(text.lower()) #Tokenize text into words\n",
        "    return [model[word] for word in words if word in model]  #extract available word vectors"
      ],
      "metadata": {
        "id": "uCVPMFypaAs_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert text into lists of word vectors\n",
        "nltk.download('punkt_tab')\n",
        "df['vectors'] = df['text'].apply(lambda x: text_to_sequence(str(x), w2v_model))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2LXm0UmaKuJ",
        "outputId": "a5a50648-f538-44c5-a134-90c620a4b709"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data into Training and Testing Sets"
      ],
      "metadata": {
        "id": "Fo9D6jTHf47N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set a fixed sentence length (Padding/Truncating)\n",
        "max_length = 30\n",
        "\n",
        "#prepare input data (X)\n",
        "X = pad_sequences(df['vectors'], maxlen=max_length, dtype='float32', padding='post', truncating='post', value=0.0)\n",
        "\n",
        "#prepare labels (y)\n",
        "y = df['label'].values\n",
        "\n",
        "#split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "nZSzXU3QaMEt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and Compile CNN Model\n"
      ],
      "metadata": {
        "id": "9mtFtaGwfw0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(max_length, embedding_dim)),\n",
        "    Dropout(0.5),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),  # إضافة L2 Regularization\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#add early stop\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoGq75xIaNpY",
        "outputId": "19db10bb-d4ab-43b3-870e-a467b76bf779"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n"
      ],
      "metadata": {
        "id": "kY25Lzz0flKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test), callbacks=[early_stop])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kg755lS-aPc4",
        "outputId": "ea425249-75ff-49c7-b956-a17482589f2c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 61ms/step - accuracy: 0.8868 - loss: 0.5632 - val_accuracy: 0.9964 - val_loss: 0.1202\n",
            "Epoch 2/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 56ms/step - accuracy: 0.9963 - loss: 0.0802 - val_accuracy: 0.9965 - val_loss: 0.0535\n",
            "Epoch 3/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 56ms/step - accuracy: 0.9977 - loss: 0.0325 - val_accuracy: 0.9974 - val_loss: 0.0256\n",
            "Epoch 4/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.9981 - loss: 0.0226 - val_accuracy: 0.9975 - val_loss: 0.0204\n",
            "Epoch 5/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.9974 - loss: 0.0205 - val_accuracy: 0.9975 - val_loss: 0.0192\n",
            "Epoch 6/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 56ms/step - accuracy: 0.9984 - loss: 0.0165 - val_accuracy: 0.9977 - val_loss: 0.0185\n",
            "Epoch 7/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 56ms/step - accuracy: 0.9987 - loss: 0.0132 - val_accuracy: 0.9974 - val_loss: 0.0173\n",
            "Epoch 8/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 56ms/step - accuracy: 0.9987 - loss: 0.0136 - val_accuracy: 0.9976 - val_loss: 0.0149\n",
            "Epoch 9/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.9989 - loss: 0.0103 - val_accuracy: 0.9976 - val_loss: 0.0146\n",
            "Epoch 10/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 56ms/step - accuracy: 0.9987 - loss: 0.0129 - val_accuracy: 0.9975 - val_loss: 0.0145\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7de46e865710>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n"
      ],
      "metadata": {
        "id": "6rprIbx5fd9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Improved CNN Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d757UMiGaRRY",
        "outputId": "d8e2006a-9ce8-450f-914f-ba4dcf4a25e5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9981 - loss: 0.0123\n",
            "Improved CNN Model Accuracy: 0.9975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#save the model\n",
        "\n"
      ],
      "metadata": {
        "id": "vkGG7TYkg6DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"cnn_model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hps06hwog5YI",
        "outputId": "33a962ee-27d8-4f05-ec20-1dfc084403eb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Predictions and Save to CSV\n"
      ],
      "metadata": {
        "id": "EyqT2bzufaJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)  #convert labels into 0 and 1\n",
        "\n",
        "predictions_df = pd.DataFrame({'predicted_label': y_pred.flatten()})\n",
        "\n",
        "predictions_df.to_csv('cnn_predictions.csv', index=False)\n",
        "\n",
        "print(\"predictions.csv is saved successfuly\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTMARFmwdh3Q",
        "outputId": "18cd218e-25cc-4f7d-e79e-1509d94cc7ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "predictions.csv is saved successfuly\n"
          ]
        }
      ]
    }
  ]
}